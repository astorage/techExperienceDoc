kafka 设计

动机

处理大公司所有的实时数据

高吞吐量，实时日志聚集

优雅处理大型数据积压，从脱机系统定期装载数据

低延迟

分区 分布式 实时处理

机器故障容错

类似数据库日志


持久性

文件系统存储  缓存消息   合理设计磁盘结构

磁盘线性写 600M/s 随机写速度是100k/s


预读和后写

提前读取多个大块的数据

将多个逻辑的写合并成一个大的物理写

线性的读磁盘 有某些情况下 比内存随机读 快


java对象很消耗内存，一般是所存数据的2倍

随着堆内存数据增多，java的垃圾回收会越来越繁琐和缓慢 


因此使用文件系统并利用页缓存 方案 比 缓存在内存中或其他结构中 要好。


我们通过自动访问所有可用的内存将使得可用的内存至少提高一倍。并可能通过存储紧凑型字节结构再次提高一倍。这将使得32G机器上高达28-32GB的缓存，并无需GC



页缓存在重启服务后仍然有效，进程缓存在内存重建的话就更快，而不用去从磁盘获取。页缓存和磁盘数据一致性（coherency）保持，由操作系统实现。

提前读 有效的需要的数据从磁盘上加载的到页缓存

这样的话设计就很简单：  内存中数据足够多时，将数据一次性flush到页缓存，就是文件系统的持久化日志，释放内存空间


这就是利用文件系统的页缓存来设计的思路


消息系统中使用的数据持久化结构通常是 每个客户端一个队列，关联btree或则随机读写数据结构，来存储消息的metadata。
btree是一个很有用的数据结构，在消息系统汇总支持各种事务和非事务语法。

btree的性能很好 是O(logn), 一般被认为是常数。

磁盘写入很慢，一次pop需要10ms 需要多次寻址，而且每次一个磁盘只能进行一次寻址，并行操作受限。因此点儿寻址就会导致很高的消耗，所以操作系统 结合很快的缓存操作和很慢的物理硬盘操作

据观察，在数据以固定缓存大小增加时，树结构的性能好于线性。 如果两倍缓存的数据增加时，树结构的性能要比线性结构慢很多。



队列持久结构可以建立在线性度和追加文件写的情况上，类似日志的情况。这种结构的优点是所有操作系统的时间复杂度是O(1)，而且读写不会相互阻塞。
这具有明显的性能优势，因为性能和数据大小完全decoupled。一个服务器利用大量的便宜、低转速、大容量的磁盘驱动。
尽管他们具有很差的寻址性能，这些驱动具有可接受的性能在大量读写的情况下，成本低，容量更大。


在没有性能损失的情况下可以访问无限制大小的虚拟磁盘空间。这也是kafka中，被消费的消息不会立即删除，会保留较长时间。这个消费端很大的灵活性



效率

This is particularly important when trying to run a centralized service that supports dozens or hundreds of applications on a centralized cluster

以前影响消息系统效率的方式，通常有2个原因：太多的小的io 操作，过多的字节拷贝

小的io问题发生在客户端和服务端，服务端持久化操作

为了避免上面的2个问题，我们的协议围绕着“message set”抽象概念建立起来的，自然的将消息分组到一起。这允许网络请求将消息分组到一起，来摊销网络来回overhead（开支）。服务端一次的将大块的消息追加的日志文件，
并且消费者每次获取大量的线性的消息块


简单的优化提升数量级的速度，批量导致更大的网络包，更大的顺序磁盘写，连续的内存块等等。这些允许kafka将大量的消息随机写流变成流向客户端线性的写

另一个影响性能的就是字节拷贝，怎么解决呢？使用标准的二进制消息格式 producer broker consumer都统一用这种格式


消息的持久化格式跟producer和consumer使用的格式一样，保持相同的格式有助于很多重要操作的优化：网络传输持久化日志块，现代的unix系统针对将数据从页缓存传输到socket进行了极度优化，linux使用sendfile 系统调用来处理。


要理解sendfile的影响，理解将数据从文件传输到socket的通用路径就很重要了

1、操作系统将数据从disk读取到内核空间的页缓存

2、应用程序从内核空间读取数据到用户空间缓存

3、应用程序将数据写回到内核空间中的一个socket缓存

4、操作系统将数据从socket缓存复制到NIC缓存，在那里数据被发送通过网络


这显然人低效，4次拷贝，2次系统调用。如果使用sendfile， 可以避免重复拷贝，允许操作系统从pagecache将数据发送直接到network，所以优化的路径，只需要最后copy到NIC buffer是必要的。


我们希望通常的使用场景是一个topic有多个客户端， 使用上面的zero-copy优化，数据只需要copy到pagecache一次，多个消费端可以重复使用，而不是数据存于内存的情况，每次都拷贝到内核。这允许消息以接近网络连接的速度被消费。


pagecache和sendfile连接意味着 kafka集群的消费者基本上都不会从disk读取数据，几乎全部从cache读取的。



sendfile和zero-copy 是java中的技术。


一般瓶颈不在cpu也不在disk，而是在网络带宽。kafka让用户压缩数据一次一条，而不需要额外的支持。这样可能导致很低的压缩率，因为很多的冗余归因于相同类型消息的重复（json中的字段名，用户在weblog中的代理，相同的string 值）

高效的压缩方法要求压缩多个消息在一起， 而不是每个消息单独压缩。

kafka支持这种高效的压缩方式，通过遍历消息sets。一批消息会被成群的组织在一起，并用这种形式发送到server。这批数据将以压缩的形式写入，在log中也保持压缩形式，在consumer端才会被解压。


kafka支持gzip和snappy的压缩协议。




kafka生产者

负载均衡
producer将数据直接发送到是leader的partition所在的broker，不需要路由层参与。为了帮助producer做到，所有的kafka节点任何时候都可以回答元数据的请求，关于哪些server是存活的，topic的partition的leader在哪。允许producer取消它的请求。

客户端控制将消息发送到哪一个partition。可以是random，实现一种随机负载均衡，也可是一些语义的分区方法。我们假设语义分区接口，允许用户指定一个key来分区，用这个key来取hash来指定到一个分区。（也可以重写分区方法，需要的话）
例如 如果选择的key是userid，同一个user的所有数据会被发送到同一个partition。这个次序允许consumer 进行locality消费。这种分群方式设计明显的是为了能在消费端进行locality处理。


异步发送

批量处理是效率的一大驱动。为了批量处理kafka producer尝试在内存中积攒数据，然后在单个请求中发出交大的批数据。可以配置批处理最大的消息个数，和最大的延迟bound。这样允许发送较多的字节数，较少的大io操作在server上。
这个buffering 是可配置的并且给了一种机制来权衡用很小的额外延迟来达到更好的吞吐量

生产者的配置和api细节在文档的某些地方可以找到。



消费者

kafka消费者以 发出fetching的请求到想要消费的leader partitions的brokers  的方式工作。consumer每次请求中指定他在log中的offset，然后获取到从offset开始的一块log（消息）。consumer因此控制消费的位置，也可以倒回去重新消费。

pull还是push

我们最初考虑的问题是 consumer从brokers拉取数据还是brokers向consumer推送数据。在这方面kafka遵循了更传统的设计，大部分消息系统都是这样的，producer将消息push到broker，然后consumer 从broker pull。一些日志中心系统，
例如Scribe和Apache Flume 使用了不一样的基于push的方式，数据是被push到下游的。两种方法各有好处，基于push的系统很难处理不同的consumer，因为broker控制传输速率。总体目标是consumer能尽可能大的速率消费；
不幸的是，在基于push的系统中，这意味着consumer 趋向于被压制，他的消费速度低于产生消息的速率。在基于pull的系统中具有更好的属性
































